{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):  \n",
    "    # lowercase\n",
    "    text=text.lower()   \n",
    "    #remove tags\n",
    "    text=re.sub(\"<!--?.*?-->\",\"\",text)  \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pawanjeetkaur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_tokens(tokens):\n",
    "    filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processtext(text, stemmer):\n",
    "    tokens = get_tokens(pre_process(text))\n",
    "    rm_stopwords = get_filtered_tokens(tokens)\n",
    "    stemTokens = stem_tokens(rm_stopwords,stemmer)\n",
    "    return stemTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run extraction/cleaning part for list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_files(files):\n",
    "    text_arr = []\n",
    "    size = len(files)\n",
    "    for i in files[:size]:\n",
    "        with open(i, 'r') as univ:\n",
    "            text = univ.read()\n",
    "            stemmer = PorterStemmer()\n",
    "            proc_txt = processtext(text, stemmer)\n",
    "            text_arr.append(proc_txt)\n",
    "    \n",
    "    return text_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating frequency objects for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_words(word, words):\n",
    "    count = 0\n",
    "    for t in words:\n",
    "        if t == word:\n",
    "            count += 1\n",
    "    return count\n",
    "    \n",
    "\n",
    "def get_word_count_total(text_each_doc):\n",
    "    freq_per_doc = []\n",
    "    size = len(text_each_doc)\n",
    "    for i in range(size):\n",
    "        words_counts = {}\n",
    "        words = text_each_doc[i]\n",
    "        for j in words:\n",
    "            try:\n",
    "                words_counts[j].add(count_words(j , words))\n",
    "            except:\n",
    "                words_counts[j] = count_words(j , words)\n",
    "        freq_per_doc.append({\"doc_id\" : i , \"freq\" : words_counts , \"doc_length\": len(words)})\n",
    "    return freq_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating tf\n",
    "\n",
    "def cal_tf(docs_info):\n",
    "    tf_scores = []\n",
    "    for doc in docs_info:\n",
    "        doc_id = doc[\"doc_id\"]\n",
    "        frequency_words_list = doc[\"freq\"]\n",
    "        for word in frequency_words_list:\n",
    "            score = frequency_words_list[word]/ doc[\"doc_length\"]\n",
    "            tf_scores.append({'doc_id': doc_id , 'word_key': word , \"tf_score\": score})\n",
    "        \n",
    "    return tf_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating idf = ln(total number of docs / number of docs with term in it)\n",
    "import math\n",
    "def cal_idf(docs_info):\n",
    "    idf_scores =[]\n",
    "\n",
    "    for doc in docs_info:\n",
    "        total_docs = len(docs_info)\n",
    "        doc_id = doc[\"doc_id\"]\n",
    "        for word in doc[\"freq\"].keys():\n",
    "            count = 1\n",
    "            for each_doc in docs_info:\n",
    "                for word_1 in each_doc[\"freq\"]:\n",
    "                    if word == word_1:\n",
    "                        count += 1   \n",
    "            score =  math.log(total_docs / count)\n",
    "            idf_scores.append({'doc_id': doc_id , 'word_key': word , \"idf_score\": score})\n",
    " \n",
    "    return idf_scores\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate tfidf \n",
    "def cal_tf_idf(tf_scores, idf_scores):\n",
    "    tfidf = {}\n",
    "    for i in tf_scores:\n",
    "        for j in idf_scores:\n",
    "            if (i[\"word_key\"] == j[\"word_key\"]) and (i[\"doc_id\"] == j[\"doc_id\"]):\n",
    "                score = i[\"tf_score\"]*j[\"idf_score\"]\n",
    "                docID = i[\"doc_id\"]\n",
    "                word = i[\"word_key\"]\n",
    "                temp = {}\n",
    "                try:\n",
    "                    temp = tfidf[docID]\n",
    "                except:\n",
    "                    tfidf[docID] = {}\n",
    "                temp[word] = score\n",
    "                tfidf[docID] = temp   \n",
    "    return tfidf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def cosine_sim(doc1, doc2):\n",
    "    sqr_sum_1 = 0\n",
    "    sqr_sum_2 = 0\n",
    "    num = 0\n",
    "    for word in doc1:\n",
    "        if word in doc2.keys():\n",
    "            tfidf1 = doc1[word]\n",
    "            tfidf2 = doc2[word]\n",
    "            num += tfidf1 * tfidf2\n",
    "            sqr_sum_1 += (tfidf1) * (tfidf1)\n",
    "            sqr_sum_2 += (tfidf2) * (tfidf2)\n",
    "    cosine_sim = num / (sqrt(sqr_sum_1) * (sqrt(sqr_sum_2)))\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between uic and mit\n",
      "0.6533202324707292\n",
      "between uic and UIUC\n",
      "0.7845752749179364\n",
      "between uic and standford\n",
      "0.778343948284832\n",
      "between uic and tesla\n",
      "0.4919936036377691\n",
      "between uic and uis\n",
      "0.7759275725537244\n"
     ]
    }
   ],
   "source": [
    "files = ['../test_1.txt', '../test_2.txt']\n",
    "\n",
    "def process_everything(files):\n",
    "    text_arr = get_words_in_files(files)\n",
    "    tot_words_in_docs = get_word_count_total(text_arr)\n",
    "    tf_score = cal_tf(tot_words_in_docs)\n",
    "    idf_score = cal_idf(tot_words_in_docs)\n",
    "    final = cal_tf_idf(tf_score, idf_score)\n",
    "    return cosine_sim(final[0], final[1])\n",
    "\n",
    "\n",
    "files = ['../UIC.txt', '../MIT.txt']\n",
    "sim_1 = process_everything(files)\n",
    "print(\"between uic and mit\")\n",
    "print(sim_1)\n",
    "\n",
    "files = ['../UIC.txt', '../UIUC.txt']\n",
    "sim_2 = process_everything(files)\n",
    "print(\"between uic and UIUC\")\n",
    "print(sim_2)\n",
    "\n",
    "files = ['../UIC.txt', '../Standford.txt']\n",
    "sim_3 = process_everything(files)\n",
    "print(\"between uic and standford\")\n",
    "print(sim_3)\n",
    "\n",
    "\n",
    "files = ['../UIC.txt', '../Tesla.txt']\n",
    "sim_4 = process_everything(files)\n",
    "print(\"between uic and tesla\")\n",
    "print(sim_4)\n",
    "\n",
    "files = ['../UIC.txt', '../UIS.txt']\n",
    "sim_5 = process_everything(files)\n",
    "print(\"between uic and uis\")\n",
    "print(sim_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacard similarity = (A intersection B ) / (A Union B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(A , B):\n",
    "    count = 0\n",
    "    for word in A:\n",
    "        if word in B:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(files):\n",
    "    text_arr = get_words_in_files(files)\n",
    "    tot_words_in_docs = get_word_count_total(text_arr)\n",
    "    \n",
    "    set_1 = tot_words_in_docs[0][\"freq\"].keys()\n",
    "    set_2 = tot_words_in_docs[1][\"freq\"].keys()\n",
    "    common_count = get_intersection(set_1 , set_2)\n",
    "    union_count = len(set_1) + len(set_2) - common_count\n",
    "    \n",
    "    jacc_sim = common_count / union_count\n",
    "    return jacc_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between uic and mit\n",
      "0.2160023446658851\n",
      "between uic and UIUC\n",
      "0.28739130434782606\n",
      "between uic and standford\n",
      "0.2661625708884688\n",
      "between uic and tesla\n",
      "0.1937479648323022\n",
      "between uic and uis\n",
      "0.2473604826546003\n"
     ]
    }
   ],
   "source": [
    "files = ['../UIC.txt', '../MIT.txt']\n",
    "sim_1 = get_jaccard_sim(files)\n",
    "print(\"between uic and mit\")\n",
    "print(sim_1)\n",
    "\n",
    "files = ['../UIC.txt', '../UIUC.txt']\n",
    "sim_2 = get_jaccard_sim(files)\n",
    "print(\"between uic and UIUC\")\n",
    "print(sim_2)\n",
    "\n",
    "files = ['../UIC.txt', '../Standford.txt']\n",
    "sim_3 = get_jaccard_sim(files)\n",
    "print(\"between uic and standford\")\n",
    "print(sim_3)\n",
    "\n",
    "\n",
    "files = ['../UIC.txt', '../Tesla.txt']\n",
    "sim_4 = get_jaccard_sim(files)\n",
    "print(\"between uic and tesla\")\n",
    "print(sim_4)\n",
    "\n",
    "files = ['../UIC.txt', '../UIS.txt']\n",
    "sim_5 = get_jaccard_sim(files)\n",
    "print(\"between uic and uis\")\n",
    "print(sim_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
